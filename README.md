# ğŸš€ ScrapifyAPI

**ScrapifyAPI** est une API REST lÃ©gÃ¨re de web scraping construite avec FastAPI et BeautifulSoup. Elle permet d'extraire des donnÃ©es structurÃ©es depuis des sites web (citations, offres d'emploi, produits, etc.) et de les exposer via une API RESTful moderne et performante.

Ce projet dÃ©montre comment construire une architecture complÃ¨te combinant **ingestion de donnÃ©es** et **couche API** â€” deux compÃ©tences essentielles en data engineering.

---

## ğŸ“‹ Table des matiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture du projet](#-architecture-du-projet)
- [Technologies utilisÃ©es](#-technologies-utilisÃ©es)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Endpoints de l'API](#-endpoints-de-lapi)
- [Structure des donnÃ©es](#-structure-des-donnÃ©es)
- [DÃ©ploiement](#-dÃ©ploiement)
- [DÃ©veloppement](#-dÃ©veloppement)
- [Contribuer](#-contribuer)
- [Licence](#-licence)

---

## âœ¨ FonctionnalitÃ©s

- ğŸŒ **Web Scraping automatisÃ©** : Extraction de citations depuis [quotes.toscrape.com](https://quotes.toscrape.com/)
- ğŸ”„ **API REST moderne** : Exposition des donnÃ©es via FastAPI avec documentation interactive
- ğŸ“Š **Validation des donnÃ©es** : Utilisation de Pydantic pour garantir la cohÃ©rence des donnÃ©es
- ğŸš€ **Performances optimales** : API asynchrone avec FastAPI
- ğŸ“– **Documentation automatique** : Swagger UI et ReDoc intÃ©grÃ©s
- â˜ï¸ **PrÃªt pour le cloud** : Configuration pour dÃ©ploiement sur Render.com
- ğŸ§¹ **Nettoyage des donnÃ©es** : Traitement et normalisation automatique des textes extraits

---

## ğŸ—ï¸ Architecture du projet

```
ScrapifyAPI/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py          # Initialisation du package
â”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e FastAPI et dÃ©finition des routes
â”‚   â”œâ”€â”€ models.py            # ModÃ¨les Pydantic pour la validation des donnÃ©es
â”‚   â”œâ”€â”€ scraper.py           # Logique de web scraping avec BeautifulSoup
â”‚   â””â”€â”€ utils.py             # Fonctions utilitaires (nettoyage de texte, etc.)
â”‚
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python du projet
â”œâ”€â”€ render.yaml             # Configuration pour dÃ©ploiement sur Render
â””â”€â”€ README.md               # Documentation du projet
```

### Description des modules

#### ğŸ“„ `app/main.py`
Fichier principal de l'application contenant :
- Configuration de l'application FastAPI
- DÃ©finition des routes et endpoints
- Gestion globale des erreurs
- MÃ©tadonnÃ©es de l'API (titre, description, version)

#### ğŸ“¦ `app/models.py`
DÃ©finit les modÃ¨les de donnÃ©es avec Pydantic :
- **Quote** : ModÃ¨le reprÃ©sentant une citation avec validation automatique
  - `quote` (str) : Le texte de la citation
  - `author` (str) : L'auteur de la citation
  - `tags` (List[str]) : Liste des tags associÃ©s

#### ğŸ•·ï¸ `app/scraper.py`
Contient la logique de web scraping :
- Connexion HTTP au site cible
- Parsing HTML avec BeautifulSoup
- Extraction des donnÃ©es structurÃ©es
- Gestion des erreurs de connexion

#### ğŸ› ï¸ `app/utils.py`
Fonctions utilitaires pour le traitement des donnÃ©es :
- `clean_text()` : Nettoie et normalise les chaÃ®nes de caractÃ¨res
- Suppression des espaces superflus et des sauts de ligne

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **[FastAPI](https://fastapi.tiangolo.com/)** (v0.100+) - Framework web moderne et rapide pour construire des APIs
- **[Uvicorn](https://www.uvicorn.org/)** - Serveur ASGI haute performance
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)** - BibliothÃ¨que de parsing HTML/XML
- **[Requests](https://requests.readthedocs.io/)** - Client HTTP simple et Ã©lÃ©gant
- **[Pydantic](https://pydantic-docs.helpmanual.io/)** - Validation des donnÃ©es avec Python type hints

---

## ğŸ“¦ PrÃ©requis

- **Python** 3.8 ou supÃ©rieur
- **pip** (gestionnaire de paquets Python)
- Connexion Internet (pour le scraping)

---

## ğŸš€ Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/Honkpehedji-Stanley/ScrapifyAPI.git
cd ScrapifyAPI
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# CrÃ©er l'environnement virtuel
python -m venv venv

# Activer l'environnement virtuel
# Sur Linux/Mac :
source venv/bin/activate

# Sur Windows :
venv\Scripts\activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

---

## ğŸ’» Utilisation

### Lancer le serveur en local

```bash
uvicorn app.main:app --reload
```

Options disponibles :
- `--reload` : RedÃ©marrage automatique lors de modifications du code (dÃ©veloppement)
- `--host 0.0.0.0` : Ã‰coute sur toutes les interfaces rÃ©seau
- `--port 8000` : SpÃ©cifier un port personnalisÃ© (8000 par dÃ©faut)

Le serveur sera accessible Ã  l'adresse : **http://127.0.0.1:8000**

### AccÃ©der Ã  la documentation interactive

Une fois le serveur lancÃ©, vous pouvez accÃ©der Ã  :

- **Swagger UI** : http://127.0.0.1:8000/docs
- **ReDoc** : http://127.0.0.1:8000/redoc

Ces interfaces vous permettent de tester directement les endpoints de l'API.

---

## ğŸ”Œ Endpoints de l'API

### 1. Route d'accueil

**GET** `/`

Endpoint de bienvenue pour vÃ©rifier que l'API fonctionne.

**RÃ©ponse :**
```json
{
  "message": "Welcome to ScrapifyAPI ğŸ‘‹"
}
```

### 2. RÃ©cupÃ©rer les citations

**GET** `/api/quotes`

RÃ©cupÃ¨re toutes les citations disponibles depuis le site web cible.

**RÃ©ponse :**
```json
[
  {
    "quote": "The world as we have created it is a process of our thinking...",
    "author": "Albert Einstein",
    "tags": ["change", "deep-thoughts", "thinking", "world"]
  },
  {
    "quote": "It is our choices, Harry, that show what we truly are...",
    "author": "J.K. Rowling",
    "tags": ["abilities", "choices"]
  }
]
```

**Codes de statut :**
- `200 OK` : RÃ©cupÃ©ration rÃ©ussie
- `500 Internal Server Error` : Erreur lors du scraping (site inaccessible, parsing Ã©chouÃ©, etc.)

---

## ğŸ“Š Structure des donnÃ©es

### ModÃ¨le Quote

```python
{
  "quote": str,      # Texte de la citation (obligatoire)
  "author": str,     # Nom de l'auteur (obligatoire)
  "tags": List[str]  # Liste des tags associÃ©s (obligatoire, peut Ãªtre vide)
}
```

**Exemple :**
```json
{
  "quote": "The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.",
  "author": "Albert Einstein",
  "tags": ["change", "deep-thoughts", "thinking", "world"]
}
```

---

## â˜ï¸ DÃ©ploiement

### DÃ©ploiement sur Render.com

Le projet inclut un fichier `render.yaml` pour un dÃ©ploiement simple sur Render.

#### Configuration (`render.yaml`)

```yaml
services:
  - type: web
    name: scrapifyapi
    env: python
    plan: free
    buildCommand: "pip install -r requirements.txt"
    startCommand: "uvicorn app.main:app --host 0.0.0.0 --port 10000"
```

#### Ã‰tapes de dÃ©ploiement :

1. CrÃ©ez un compte sur [Render.com](https://render.com)
2. Connectez votre dÃ©pÃ´t GitHub
3. CrÃ©ez un nouveau "Web Service"
4. Render dÃ©tectera automatiquement le fichier `render.yaml`
5. Cliquez sur "Deploy"

Votre API sera accessible via une URL fournie par Render (ex: `https://scrapifyapi.onrender.com`)

### DÃ©ploiement sur d'autres plateformes

Le projet peut Ã©galement Ãªtre dÃ©ployÃ© sur :
- **Heroku** : Ajoutez un `Procfile` avec `web: uvicorn app.main:app --host 0.0.0.0 --port $PORT`
- **Railway** : Utilisez la dÃ©tection automatique
- **Google Cloud Run** : CrÃ©ez un `Dockerfile`
- **AWS Elastic Beanstalk** : Utilisez la configuration Python

---

## ğŸ”§ DÃ©veloppement

### Tester l'API avec curl

```bash
# Test de la route d'accueil
curl http://127.0.0.1:8000/

# RÃ©cupÃ©ration des citations
curl http://127.0.0.1:8000/api/quotes
```

### Tester avec Python

```python
import requests

# RÃ©cupÃ©rer les citations
response = requests.get("http://127.0.0.1:8000/api/quotes")
quotes = response.json()

for quote in quotes:
    print(f"ğŸ“– {quote['quote']}")
    print(f"âœï¸  â€” {quote['author']}")
    print(f"ğŸ·ï¸  Tags: {', '.join(quote['tags'])}\n")
```

### Ajouter un nouveau site Ã  scraper

Pour scraper un nouveau site :

1. CrÃ©ez une nouvelle fonction dans `app/scraper.py`
2. DÃ©finissez un nouveau modÃ¨le dans `app/models.py` si nÃ©cessaire
3. Ajoutez un nouveau endpoint dans `app/main.py`

**Exemple :**

```python
# Dans app/scraper.py
def scrape_jobs():
    # Votre logique de scraping
    pass

# Dans app/main.py
@app.get("/api/jobs", response_model=List[Job])
def get_jobs():
    try:
        jobs = scrape_jobs()
        return jobs
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ¤ Contribuer

Les contributions sont les bienvenues ! Voici comment contribuer :

1. **Forkez** le projet
2. CrÃ©ez votre branche de fonctionnalitÃ© (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une **Pull Request**

### Bonnes pratiques

- Suivez les conventions PEP 8 pour le code Python
- Ajoutez des tests pour les nouvelles fonctionnalitÃ©s
- Documentez les nouvelles fonctions et endpoints
- Mettez Ã  jour le README si nÃ©cessaire

---

## ğŸ“ AmÃ©liorations futures

- [ ] Ajout d'un cache Redis pour amÃ©liorer les performances
- [ ] Pagination des rÃ©sultats pour les grandes collections
- [ ] Authentification API avec tokens JWT
- [ ] Support de plusieurs sources de donnÃ©es
- [ ] Export des donnÃ©es en CSV/JSON
- [ ] Scraping asynchrone pour de meilleures performances
- [ ] Base de donnÃ©es pour stocker l'historique des donnÃ©es
- [ ] Rate limiting pour Ã©viter la surcharge du serveur
- [ ] SystÃ¨me de logs avancÃ©
- [ ] Tests unitaires et d'intÃ©gration

---

## ğŸ“„ Licence

Ce projet est un projet Ã©ducatif et de dÃ©monstration. Libre d'utilisation et de modification.

---

## ğŸ‘¤ Auteur

**Stanley Honkpehedji**

- GitHub: [@Honkpehedji-Stanley](https://github.com/Honkpehedji-Stanley)

---

## âš ï¸ Avertissement

Ce projet est Ã  but Ã©ducatif. Lors du scraping de sites web :
- Respectez les conditions d'utilisation des sites web
- VÃ©rifiez le fichier `robots.txt` du site cible
- Ne surchargez pas les serveurs avec des requÃªtes excessives
- Respectez la propriÃ©tÃ© intellectuelle des donnÃ©es

---

## ğŸ“š Ressources utiles

- [Documentation FastAPI](https://fastapi.tiangolo.com/)
- [Documentation BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Guide Pydantic](https://pydantic-docs.helpmanual.io/)
- [Web Scraping Best Practices](https://www.scrapingbee.com/blog/web-scraping-best-practices/)

---

<div align="center">
  Fait avec â¤ï¸ et Python ğŸ
</div>
